{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c28dff8-7cc2-4edd-b2f3-432d89fdec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b93fda6-a007-445c-8e26-bf68c54ed584",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_model = api.load('word2vec-google-news-300')\n",
    "sample_word2vec_embedding = v2w_model['computer'];\n",
    "\n",
    "glove_model = api.load('glove-twitter-25')\n",
    "sample_glove_embedding = glove_model['computer'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfe15c2-9b02-45d9-a7c9-3b8feb844cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(line):\n",
    "    word, tag = line.split('\\t')\n",
    "    #removing those does a bit more bad than good i think.\n",
    "    #if word[0] in [\"#\", \"~\", \"-\", \"@\"]:\n",
    "        #word = word[1:]\n",
    "     #   word = word[0]\n",
    "    if word[:4] == \"http\":\n",
    "        word = \"http\"\n",
    "    tag = tag[:-1]\n",
    "    tag = 1 if tag != 'O' else 0\n",
    "    try:\n",
    "        emb_word1 = np.append(embedding1[word], np.zeros(1))\n",
    "    except:\n",
    "        try:\n",
    "            emb_word1 = np.append(embedding1[word.capitalize()], np.zeros(1))\n",
    "        except:\n",
    "            if word in extras1.keys():\n",
    "                new_embed = extras1[word]\n",
    "            else:\n",
    "                #new_embed =  np.append(torch.normal(mean=torch.zeros(300), std=5000*torch.ones(300)), 100000*np.ones(1))\n",
    "                new_embed =  np.append(np.random.normal(loc=0, scale=5000, size=300), 100000*np.ones(1))\n",
    "                #new_embed =  np.append(torch.randn(300), 100000*np.ones(1))\n",
    "                extras1[word] = new_embed\n",
    "                if tag == 1:\n",
    "                    print(word)\n",
    "            emb_word1 = np.append(np.zeros(300, dtype=\"float32\"), 100000*np.ones(1))\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        emb_word2 = np.append(embedding2[word], np.zeros(1))\n",
    "    except:\n",
    "        if word in extras2.keys():\n",
    "                new_embed = extras2[word]\n",
    "        else:\n",
    "            #new_embed =  np.append(torch.normal(mean=torch.zeros(25), std=5000*torch.ones(25)), 100000*np.ones(1))\n",
    "            new_embed =  np.append(np.random.normal(loc=0, scale=5000, size=25), 100000*np.ones(1))\n",
    "            #new_embed = np.append(torch.randn(25), 100000*np.ones(1))\n",
    "            extras2[word] = new_embed\n",
    "            if tag == 1:\n",
    "                print(word)\n",
    "        emb_word2 = np.append(np.zeros(25, dtype=\"float32\"), 100000*np.ones(1))\n",
    "    emb_word = np.append(emb_word1, emb_word2)\n",
    "    return emb_word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0769a6-80f7-4a36-95ed-664a8e4fd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = v2w_model\n",
    "embedding2 = glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31784c77-8110-4296-9a6b-c42e1d1776c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras1 = dict()\n",
    "extras2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c390cb0-0184-4ab3-b5ce-210de43eebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        curr_sen = []\n",
    "        curr_labels = []\n",
    "        for line in file.readlines():\n",
    "            if len(line.split('\\t')) > 1  and line != '\\t' and line != '\\n' and len(line.split('\\t')[-1]) > 1:\n",
    "                emb_word, tag = process_word(line)\n",
    "                curr_sen.append(emb_word)\n",
    "                curr_labels.append(tag)\n",
    "            else:\n",
    "                sentences.append(curr_sen[::])\n",
    "                labels.append(curr_labels[::])\n",
    "                curr_sen = []\n",
    "                curr_labels = []  \n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77228e0d-c8c8-4c58-afd7-eb2364a33699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2w_model[\"http\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc8592-2dd6-49a7-b85d-e55efd7c91e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dd16be-801b-4806-b2bf-5445e05a8b88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahfa\n",
      "Pxleyes\n",
      "pxleyes\n",
      "4Dbling\n",
      "4dbling\n",
      "#BattlestarGalactica\n",
      "#battlestargalactica\n",
      "#StarWars\n",
      "#starwars\n",
      "#TheCloneWars\n",
      "#theclonewars\n",
      "Wintor\n",
      "wintor\n",
      "G24\n",
      "g24\n",
      "#Padres\n",
      "#padres\n",
      "@ravenalexis\n",
      "@ravenalexis\n",
      "@jessejane\n",
      "@jessejane\n",
      "@rileysteele\n",
      "@rileysteele\n",
      "@kayden_kross\n",
      "@kayden_kross\n",
      "justthen\n",
      "justthen\n",
      "gdns\n",
      "Savoirfaire\n",
      "savoirfaire\n",
      "Jessum\n",
      "jessum\n",
      "-Roald\n",
      "-roald\n",
      "107\n",
      "107\n",
      "V-I-K-I-N-G-S\n",
      "v-i-k-i-n-g-s\n",
      "holyroodhouse\n",
      "maunalua\n",
      "persuaders\n",
      "solich\n",
      "AstronomersWithoutBorders\n",
      "astronomerswithoutborders\n",
      "rodanthe\n",
      "Nijverdal\n",
      "JUTHTIN\n",
      "juthtin\n",
      "BEAVERRRR\n",
      "beaverrrr\n",
      "BPA-Free\n",
      "branchout\n",
      "dabigatran\n",
      "M&amp;M\n",
      "reivers\n",
      "EuroVPS\n",
      "eurovps\n",
      "#Vh1\n",
      "#vh1\n",
      "-Larry\n",
      "-larry\n",
      "sd2\n",
      "5800\n",
      "5800\n",
      "#BB11\n",
      "#bb11\n",
      "Flikken\n",
      "jannus\n",
      "suenalo\n",
      "SHAKEMODE\n",
      "shakemode\n",
      "30stm\n",
      "30stm\n",
      "Erenice\n",
      "erenice\n",
      "ghostland\n",
      "ps3\n",
      "360\n",
      "360\n",
      "widro\n",
      "Knighttime\n",
      "knighttime\n",
      "beleskey\n",
      "iphone4\n",
      "amenabar\n",
      "W/Rachel\n",
      "w/rachel\n",
      "2008\n",
      "2008\n",
      "r2\n",
      "Futoosh\n",
      "futoosh\n",
      "rimutakas\n",
      "*Gwen\n",
      "*gwen\n",
      "Cachupas\n",
      "cachupas\n",
      "#JESUS\n",
      "#jesus\n",
      "Mary's.\n",
      "mary's.\n",
      "al-Mallohi\n",
      "al-mallohi\n",
      "calaman\n",
      "Robinhoods\n",
      "robinhoods\n",
      "*Boston*\n",
      "*boston*\n",
      "shpongle\n",
      "posford\n",
      "U21\n",
      "u21\n",
      "tipitina\n",
      "Cyber-Ark\n",
      "cyber-ark\n",
      "61\n",
      "61\n",
      "teaze\n",
      "murkowski\n",
      "mariaelena\n",
      "#Astros\n",
      "#astros\n",
      "K-On\n",
      "ngmoco\n",
      "sfitzy93\n",
      "sfitzy93\n",
      "AT&amp;T\n",
      "N.E.R.D.\n",
      "Vordingborg\n",
      "vordingborg\n",
      "kamenetz\n",
      "Swayzes\n",
      "swayzes\n",
      "glenveagh\n",
      "-Coco\n",
      "-coco\n",
      "uflex\n",
      "shange\n",
      "338\n",
      "338\n",
      "Wippenberg\n",
      "wippenberg\n",
      "Guestmix\n",
      "Cabio\n",
      "R'lyeh,\n",
      "r'lyeh,\n",
      "Tdot\n",
      "Taorminaa\n",
      "taorminaa\n",
      "wertheim\n",
      "o.d.b.\n",
      "o.d.b.\n",
      "Brilmayer\n",
      "brilmayer\n",
      "Vuelter\n",
      "vuelter\n",
      "#Denver\n",
      "#denver\n",
      "Patchogue)\n",
      "patchogue)\n",
      "Seanzie\n",
      "seanzie\n",
      "Julliet\n",
      "Park-\n",
      "park-\n",
      "CocoFunka\n",
      "cocofunka\n",
      "Indiesent\n",
      "indiesent\n",
      "lhs\n",
      "wrawby\n",
      "#Dallas\n",
      "#dallas\n",
      "a7x\n",
      "@Cromwell\n",
      "@cromwell\n",
      "NFL)\n",
      "nfl)\n",
      "@Jessica_Chobot\n",
      "@jessica_chobot\n",
      "Float-Ellicott\n",
      "float-ellicott\n",
      "blakehurst\n",
      "PROFIONAL\n",
      "profional\n",
      "4.8\n",
      "4.8\n",
      "-Rumi\n",
      "-rumi\n",
      "TIFO\n",
      "s8\n",
      "Kadrey\n",
      "kadrey\n",
      "caliman\n",
      "Nex-Tech\n",
      "nex-tech\n",
      "CampaignMonitor\n",
      "campaignmonitor\n",
      "Fureta\n",
      "fureta\n",
      "Kwiruka\n",
      "kwiruka\n",
      "bcat\n",
      "jerraud\n",
      "Baveh\n",
      "baveh\n",
      "Bennett-\n",
      "bennett-\n",
      "Unnie\n",
      "~Wayne\n",
      "~wayne\n",
      "'Rickey\n",
      "'rickey\n",
      "@SanderVanDoorn\n",
      "@sandervandoorn\n",
      "@swedishousemfia\n",
      "@swedishousemfia\n",
      "mjd\n",
      "Newcastle-Upon-Tyne\n",
      "newcastle-upon-tyne\n",
      "Hyna\n",
      "canwest\n",
      "finese\n",
      "finese\n",
      "Kick-Ass\n",
      "techland\n",
      "minohd\n",
      "Lounge22\n",
      "lounge22\n",
      "pavonia\n",
      "lecompte\n",
      "#Cagayan\n",
      "#cagayan\n",
      "#NewDelhi\n",
      "#newdelhi\n",
      "~William\n",
      "~william\n",
      "snapchat\n",
      "Hero3+\n",
      "hero3+\n",
      "3PCS\n",
      "3pcs\n",
      "Queenx\n",
      "queenx\n",
      "M1330\n",
      "m1330\n",
      "#laptop\n",
      "#laptop\n",
      "#Steelers\n",
      "#steelers\n",
      "Khalen\n",
      "khalen\n",
      "GT-R\n",
      "m3\n",
      "Bowlounge\n",
      "bowlounge\n",
      "JabberDuck\n",
      "#Southie\n",
      "#southie\n",
      "Cascades-WA\n",
      "cascades-wa\n",
      "briarcrest\n",
      "Berahino\n",
      "ps4\n",
      "#Aquarians\n",
      "#aquarians\n",
      "#Orlando\n",
      "#orlando\n",
      "#Nexus6\n",
      "#nexus6\n",
      "AG-HMC40P\n",
      "ag-hmc40p\n",
      "3mos\n",
      "3mos\n",
      "avccam\n",
      "OaklandArtsSchool\n",
      "oaklandartsschool\n",
      "501\n",
      "501\n",
      "D55\n",
      "d55\n",
      "Jaket\n",
      "Terps-Illini\n",
      "terps-illini\n",
      "Mindsweep\n",
      "mindsweep\n",
      "Crobot\n",
      "crobot\n",
      "KathNiel\n",
      "Mark's.\n",
      "mark's.\n",
      "-Velasquez-Manoff\n",
      "-velasquez-manoff\n",
      "nadt\n",
      "jaket\n",
      "SK-II\n",
      "PA++\n",
      "pa++\n",
      "30g\n",
      "30g\n",
      "chicksen\n",
      "Mackail-Smith\n",
      "mackail-smith\n",
      "Messarounds\n",
      "messarounds\n",
      "H.E\n",
      "Lifescript\n",
      "lifescript\n",
      "nivolumab\n",
      "nivolumab\n",
      "Nandi-Kabras\n",
      "nandi-kabras\n",
      "#Buckeyes\n",
      "#buckeyes\n",
      "#Berlinale\n",
      "#berlinale\n",
      "Hatake\n",
      "Snapchat\n",
      "gx1\n",
      "32gb\n",
      "32gb\n",
      "Kongers\n",
      "kongers\n",
      "7s\n",
      "PESAWAR\n",
      "pesawar\n",
      "16GB\n",
      "16gb\n",
      "#HouseofCards\n",
      "#houseofcards\n",
      "mosier\n",
      "Capri-sun\n",
      "#DaysofFuturePast\n",
      "#daysoffuturepast\n",
      "#Shootingstar\n",
      "#shootingstar\n",
      "Subbo\n",
      "subbo\n",
      "WB350F\n",
      "wb350f\n",
      "#Longchamp\n",
      "#longchamp\n",
      "#KansasCity\n",
      "#kansascity\n",
      "MO-Kansas\n",
      "mo-kansas\n",
      "ouat\n",
      "sikkimese\n",
      "#ClubLacura\n",
      "#clublacura\n",
      "BTOB\n",
      "Ilhoon\n",
      "Beenzino\n",
      "-Cuba\n",
      "-cuba\n",
      "NoondaybyTracey\n",
      "noondaybytracey\n",
      "loveman\n",
      "frissora\n",
      "AppleMagazine\n",
      "applemagazine\n",
      "makonnen\n",
      "Roshe's!\n",
      "roshe's!\n",
      "#Theatre\n",
      "#theatre\n",
      "#Wax\n",
      "#wax\n",
      "#Bangkok\n",
      "#bangkok\n",
      "#Thailand\n",
      "#thailand\n",
      "#jets\n",
      "#jets\n",
      "#marksanchez\n",
      "#marksanchez\n",
      "#JOEGIFTED\n",
      "#joegifted\n",
      "#BIGBEN\n",
      "#bigben\n",
      "#TheRAC\n",
      "#therac\n",
      "erron\n",
      "parasut\n",
      "black-blue\n",
      "black-blue\n",
      "#China\n",
      "#china\n",
      "72F7210\n",
      "72f7210\n",
      "9556\n",
      "9556\n",
      "#Spurs\n",
      "#spurs\n",
      "#Marriott\n",
      "#marriott\n",
      "#Bengals\n",
      "#bengals\n",
      "#Patriots\n",
      "#patriots\n",
      "lantic\n",
      "ps2\n",
      "TVPS\n",
      "tvps\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels = load_data(\"data/train.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27dc3354-4357-464d-a1a5-a1c16849ec61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaryJs\n",
      "maryjs\n",
      "hangwani\n",
      "hangwani\n",
      "Pyrion\n",
      "pyrion\n",
      "jedidiah\n",
      "demeyere\n",
      "Matfer\n",
      "matfer\n",
      "bourgeat\n",
      "keyons\n",
      "keyons\n",
      "ZWEIBRUCKEN\n",
      "zweibrucken\n",
      "renallo\n",
      "renallo\n",
      "spider-man\n",
      "grian\n",
      "chandu\n",
      "kunjana\n",
      "kunjana\n",
      "Sly-\n",
      "sly-\n",
      "itx\n",
      "ozee\n",
      "uZalo\n",
      "uzalo\n",
      "threadbanger\n",
      "abq\n",
      "krithika\n",
      "Billdo\n",
      "billdo\n",
      "serpintine\n",
      "serpintine\n",
      "ROOOOOOOOOOAAAAAAAAAAAAAAAAAAAAAAAAADDDDDDDDDDDDDD\n",
      "rooooooooooaaaaaaaaaaaaaaaaaaaaaaaaadddddddddddddd\n",
      "Didga\n",
      "didga\n",
      "Ssundee\n",
      "ssundee\n",
      "Mortdecai\n",
      "mortdecai\n",
      "eslastic\n",
      "eslastic\n",
      "Capwin\n",
      "capwin\n",
      "Vinkare\n",
      "vinkare\n",
      "Herbalisti\n",
      "herbalisti\n",
      "6531\n",
      "6531\n",
      "Bulbulay\n",
      "bulbulay\n",
      "900\n",
      "900\n",
      "Troydan\n",
      "troydan\n",
      "Trumpty\n",
      "trumpty\n",
      "ishawna\n",
      "ishawna\n",
      "edubes\n",
      "edubes\n",
      "Birdtalker\n",
      "birdtalker\n",
      "DeFrozen\n",
      "defrozen\n",
      "Germini\n",
      "germini\n",
      "fouseytube\n",
      "LibraTv\n",
      "libratv\n",
      "Padikall\n",
      "padikall\n",
      "xman\n",
      "Daddarios\n",
      "daddarios\n",
      "Mcchoke\n",
      "mcchoke\n",
      "Kurzgesagt\n",
      "kurzgesagt\n",
      "overwatch\n",
      "ManVsGame\n",
      "manvsgame\n",
      "rickyxans\n",
      "rickyxans\n",
      "Morphe\n",
      "morphe\n",
      "yilong\n",
      "fedorova\n",
      "trifonov\n",
      "Michter\n",
      "michter\n",
      "viralfeedvideonetwork\n",
      "viralfeedvideonetwork\n",
      "Jonyeee\n",
      "jonyeee\n",
      "1993\n",
      "1993\n",
      "idubbbtv\n",
      "idubbbtv\n",
      "Sinophiles\n",
      "sinophiles\n",
      "ranallo\n",
      "AtwoodGames\n",
      "atwoodgames\n",
      "Skhumba\n",
      "skhumba\n",
      "kgomotso\n",
      "jackbox\n",
      "jackbox\n",
      "Mahary\n",
      "mahary\n",
      "Chik-\n",
      "chik-\n",
      "Fil-\n",
      "fil-\n",
      "raggedydan\n",
      "raggedydan\n",
      "Disnney\n",
      "disnney\n",
      "iDubbbz\n",
      "idubbbz\n",
      "hanamura\n",
      "tailler\n",
      "Princeboss\n",
      "princeboss\n",
      "rebelism\n",
      "rebelism\n",
      "Xiaomi\n",
      "herge\n",
      "Boss-\n",
      "boss-\n",
      "Yachty\n",
      "yachty\n",
      "saltmining\n",
      "saltmining\n",
      "Bagdanovich\n",
      "bagdanovich\n",
      "Keyshot\n",
      "keyshot\n",
      "MaMlambo\n",
      "mamlambo\n",
      "nosipho\n",
      "Pac-\n",
      "pac-\n",
      "MysteryGuitarMan\n",
      "mysteryguitarman\n",
      "Dreymond\n",
      "dreymond\n",
      "LeQueen\n",
      "lequeen\n",
      "RWBY\n",
      "Kingda\n",
      "Musicallys\n",
      "musicallys\n",
      "daish\n",
      "AirPods\n",
      "airpods\n",
      "Laurex\n",
      "laurex\n",
      "Bradbury-\n",
      "bradbury-\n",
      "Riceball\n",
      "riceball\n",
      "KWESTA\n",
      "woodprix\n",
      "woodprix\n",
      "Airpods\n",
      "pouler\n",
      "pouler\n",
      "airpods\n",
      "Muselk\n",
      "muselk\n",
      "Ryzen\n",
      "ryzen\n",
      "Archakam\n",
      "archakam\n",
      "koops\n",
      "mini-\n",
      "mini-\n",
      "Cancoon\n",
      "cancoon\n",
      "NoBiggieTV\n",
      "nobiggietv\n",
      "Gwil\n",
      "gwil\n",
      "ougie\n",
      "ougie\n",
      "shokugeki\n",
      "shokugeki\n",
      "ldshadowlady\n",
      "ldshadowlady\n",
      "aphmau\n",
      "aphmau\n",
      "888\n",
      "888\n",
      "baironpuria\n",
      "baironpuria\n",
      "Dubbzï»¿\n",
      "dubbzï»¿\n",
      "Discivery\n",
      "discivery\n"
     ]
    }
   ],
   "source": [
    "dev_sentences, dev_labels = load_data(\"data/dev.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf3fe26-11d2-4e38-bc22-e874963c6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contex_embeddings(sentences):\n",
    "    contex_embs = []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)):\n",
    "            if i == 0:\n",
    "                if len(sent) == 1:\n",
    "                    contex_emb = np.concatenate((sent[i], sent[i], sent[i]))\n",
    "                elif len(sent) == 2:\n",
    "                    contex_emb = np.concatenate((sent[i], sent[i], sent[i+1]))\n",
    "                else:\n",
    "                    contex_emb = np.concatenate((sent[i], sent[i+1], sent[i+2]))\n",
    "            elif i == len(sent) - 1:\n",
    "                if len(sent) == 2:\n",
    "                    contex_emb = np.concatenate((sent[i], sent[i], sent[i-1]))\n",
    "                else:\n",
    "                    contex_emb = np.concatenate((sent[i], sent[i-1], sent[i-2]))\n",
    "            else:\n",
    "                contex_emb = np.concatenate((sent[i], sent[i-1], sent[i+1]))\n",
    "            contex_embs.append(contex_emb)\n",
    "    return np.array(contex_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6109fddf-ff4e-450c-80f8-4847d228be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_flat = [item for sublist in train_labels for item in sublist]\n",
    "dev_labels_flat = [item for sublist in dev_labels for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aab014e-3b04-443f-8ab4-16cdee3054fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_flat = [item for sublist in train_sentences for item in sublist]\n",
    "dev_sentences_flat = [item for sublist in dev_sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad8d371-26ad-425c-8bff-112af1bd5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_flat = contex_embeddings(train_sentences)\n",
    "dev_context_flat = contex_embeddings(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36ba204d-3b8c-4c68-910d-5904d5f77b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=1)\n",
    "model_knn.fit(train_sentences_flat, train_labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b89864-37e9-47ae-8e54-4371b474184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepa\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5803161652218256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_dev_knn = model_knn.predict(dev_sentences_flat)\n",
    "f1_score(dev_labels_flat, predict_dev_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e7f90-b958-40db-a889-8b788617d5da",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "697f0dab-5e87-48d0-b9af-a6c848475caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a79e2d2-88e7-4097-a3b7-2f6d8a094462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, embeddings_flat, labels_flat):\n",
    "        self.embeddings = embeddings_flat\n",
    "        self.labels = labels_flat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.embeddings[idx], self.labels[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56fb2931-2038-4175-ba0e-8ee1002bf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = torch.tensor(np.array(train_context_flat)).float()\n",
    "dev_emb = torch.tensor(np.array(dev_context_flat)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667c8a77-837c-4457-9bf2-0a3a8dff5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = torch.tensor(np.array(train_sentences_flat)).float()\n",
    "dev_emb = torch.tensor(np.array(dev_sentences_flat)).float()\n",
    "train_labels_flat = torch.tensor(np.array(train_labels_flat))\n",
    "dev_labels_flat = torch.tensor(np.array(dev_labels_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c7c49c-523e-460f-a704-fc772ac2c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TweetsDataset(train_emb, train_labels_flat)\n",
    "dev_dataset = TweetsDataset(dev_emb, dev_labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99d61611-6cec-47e5-a1a4-82fd8b36f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eacff56-f9c0-4f63-875d-7fe1fe949b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9d758f0-82d6-4404-aa9e-c98aa6b691e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a93e2c69-9fe7-4278-9246-702521d74550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=327):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.first_layer = nn.Linear(input_dim, 200)\n",
    "        #self.norm = nn.BatchNorm1d(500)\n",
    "        self.final_layer = nn.Linear(200, 1)\n",
    "        #self.alternate_layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        x = self.first_layer(embeddings)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.final_layer(x)\n",
    "        #x = self.alternate_layer(embeddings)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e95c711-cbd3-4de9-9f97-5324a2ff9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epochs, train_loader, test_loader):\n",
    "    our_net = SentimentNN()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        our_net = our_net.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(4))\n",
    "    optimizer = torch.optim.Adam(our_net.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        our_net.train()\n",
    "        for i, (embeddings, labels) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                embeddings = embeddings.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            outputs = our_net(embeddings).view(-1)\n",
    "            loss = criterion(outputs,labels)\n",
    "            #loss = \n",
    "            loss.backward()\n",
    "            if (i+1) % 2 == 0 or (i+1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            if (i+1) % 300 == 0:\n",
    "                precision, recall = f1_stats(outputs, labels)\n",
    "                print(f\"precision is {precision} and recall is {recall}\")\n",
    "                #print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                #       %(epoch+1, epochs, i+1,\n",
    "                #         len(train_dataset)//batch_size, loss.data))\n",
    "        our_net.eval()\n",
    "        acc = accuracy_calc(test_loader, our_net)\n",
    "        print(acc)\n",
    "        print(f1_calc(train_loader, our_net))\n",
    "        print(f1_calc(test_loader, our_net))\n",
    "    return accuracy_calc(test_loader, our_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0272ea74-bfd8-431b-891d-2ae3596c3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_stats(outputs, labels):\n",
    "    outputs[outputs>0] = 1\n",
    "    outputs[outputs < 0] = 0\n",
    "    preds = outputs\n",
    "    TP = preds @ labels\n",
    "    FP = preds @ (1 - labels)\n",
    "    FN = (1-preds) @ labels\n",
    "    return round(float(TP / (TP + FP)), 2), round(float(TP / (TP + FN)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96bb3053-519a-4a6c-9c24-872fc3ff059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_preds):\n",
    "    return (y_true == y_preds).sum().item() / y_true.size(0)\n",
    "\n",
    "def accuracy_calc(loader, net):\n",
    "    i = sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # Turn off the gradients\n",
    "    with torch.no_grad():\n",
    "        # Loop through all of the validation set\n",
    "        for embeddings, labels in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                embeddings = embeddings.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                \n",
    "            outputs = net(embeddings).view(-1)\n",
    "            outputs[outputs>0] = 1\n",
    "            outputs[outputs < 0] = 0\n",
    "            preds = outputs\n",
    "            sum_accuracy += get_accuracy(labels, preds)\n",
    "            sum_loss += criterion(outputs,labels)\n",
    "            i += 1\n",
    "    total_accuracy = sum_accuracy / i\n",
    "    return (total_accuracy, sum_loss / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2cbfbb-9be9-4bc5-817f-830d33fffe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calc(loader, net):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Turn off the gradients\n",
    "    with torch.no_grad():\n",
    "        # Loop through all of the validation set\n",
    "        for embeddings, labels in loader:\n",
    "            if torch.cuda.is_available():\n",
    "                embeddings = embeddings.to(device)\n",
    "                \n",
    "            outputs = net(embeddings).view(-1)\n",
    "            outputs[outputs>0] = 1\n",
    "            outputs[outputs < 0] = 0\n",
    "            preds = outputs.tolist()\n",
    "            labels = labels.tolist()\n",
    "            all_preds += preds\n",
    "            all_labels += labels\n",
    "    #print([(true, pred) for true, pred in zip(all_labels, all_preds)])\n",
    "    return round(f1_score(all_labels, all_preds), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e100a12-c80e-4e17-80f5-c8da9b2175b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.89 and recall is 0.89\n",
      "(0.9449233921381267, tensor(0.6907, device='cuda:0'))\n",
      "0.53\n",
      "0.66\n",
      "precision is 0.56 and recall is 0.71\n",
      "(0.948135788207666, tensor(0.6880, device='cuda:0'))\n",
      "0.65\n",
      "0.66\n",
      "precision is 0.69 and recall is 0.69\n",
      "(0.9487846850829782, tensor(0.6872, device='cuda:0'))\n",
      "0.66\n",
      "0.65\n",
      "precision is 0.56 and recall is 0.83\n",
      "(0.950025432954454, tensor(0.6865, device='cuda:0'))\n",
      "0.67\n",
      "0.66\n",
      "precision is 0.77 and recall is 0.77\n",
      "(0.9507183026949425, tensor(0.6862, device='cuda:0'))\n",
      "0.68\n",
      "0.66\n",
      "precision is 0.92 and recall is 0.85\n",
      "(0.9534897816568976, tensor(0.6842, device='cuda:0'))\n",
      "0.7\n",
      "0.67\n",
      "precision is 0.56 and recall is 0.83\n",
      "(0.7921402663091788, tensor(0.7849, device='cuda:0'))\n",
      "0.34\n",
      "0.32\n",
      "precision is 0.88 and recall is 0.78\n",
      "(0.9533008171822189, tensor(0.6841, device='cuda:0'))\n",
      "0.71\n",
      "0.67\n",
      "precision is 0.6 and recall is 1.0\n",
      "(0.9529419035259105, tensor(0.6841, device='cuda:0'))\n",
      "0.72\n",
      "0.66\n",
      "precision is 0.55 and recall is 0.75\n",
      "(0.9517451285196119, tensor(0.6848, device='cuda:0'))\n",
      "0.73\n",
      "0.65\n",
      "precision is 0.5 and recall is 0.89\n",
      "(0.9525449592835039, tensor(0.6847, device='cuda:0'))\n",
      "0.72\n",
      "0.66\n",
      "precision is 0.33 and recall is 0.78\n",
      "(0.9525009864183268, tensor(0.6844, device='cuda:0'))\n",
      "0.73\n",
      "0.66\n",
      "precision is 0.5 and recall is 0.75\n",
      "(0.9514301877284804, tensor(0.6851, device='cuda:0'))\n",
      "0.74\n",
      "0.65\n",
      "precision is 0.57 and recall is 0.8\n",
      "(0.9515561640449329, tensor(0.6852, device='cuda:0'))\n",
      "0.74\n",
      "0.66\n",
      "precision is 0.55 and recall is 0.67\n",
      "(0.9516191522031591, tensor(0.6850, device='cuda:0'))\n",
      "0.75\n",
      "0.66\n",
      "precision is 0.85 and recall is 0.69\n",
      "(0.9511782350955754, tensor(0.6849, device='cuda:0'))\n",
      "0.76\n",
      "0.65\n",
      "precision is 0.62 and recall is 0.62\n",
      "(0.9507373179879914, tensor(0.6856, device='cuda:0'))\n",
      "0.75\n",
      "0.65\n",
      "precision is 0.64 and recall is 0.64\n",
      "(0.9496035311399189, tensor(0.6864, device='cuda:0'))\n",
      "0.74\n",
      "0.65\n",
      "precision is 0.4 and recall is 1.0\n",
      "(0.7908995184377031, tensor(0.7849, device='cuda:0'))\n",
      "0.36\n",
      "0.31\n",
      "precision is 0.43 and recall is 0.75\n",
      "(0.9511782350955754, tensor(0.6850, device='cuda:0'))\n",
      "0.76\n",
      "0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9511782350955754, tensor(0.6850, device='cuda:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.backends.cudnn.benchmark = True\n",
    "num_epochs = 20\n",
    "train_and_test(num_epochs, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d4290-cad4-4551-9dc7-9569caddfdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25bcbfa1-d18c-423a-b331-c9fe0bedc916",
   "metadata": {},
   "source": [
    "#### the best model yet was 1 hidden layer of 100 neurons, normal loss (without weights even), regular adam, 10 iterations, the 357 dim embeddings (no contex) and batch size of 64\n",
    "#### it got 0.64 on test and 0.63 on train\n",
    "#### was able to reproduce with pos weight = 5, also 4\n",
    "#### it seems to be more stable the less neurons it has (still good results at 30 neurons)\n",
    "#### got 0.66 on test one time with only 12 neurons\n",
    "#### apparently the network gain up to 0.6 f1 without any hidden layers @ activations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb0282-f59d-4e54-b84d-700fd3ababdc",
   "metadata": {},
   "source": [
    "#### Update - doing lower only for the glove embeddings (25 dim) did a wondeful job for the stability of the netwrok - keep the lower!\n",
    "#### now with the capitalize trick for the w2v, were able to get 0.67 f1 on test\n",
    "#### Tried removing #'s but it seems to hurt the knn a little bit for some reason. the network doesn't mind though.\n",
    "#### For some reason there appears to be contradictions in the labeling of the train set (atleast) on words that start with # (at least). that would explain the drop in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0bb78-58ae-499a-b94d-c8ca7479c8e6",
   "metadata": {},
   "source": [
    "#### Basiclly we have 2 options for LSTM:\n",
    "#### The first is not use batches - we can keep the setences as they are and simply train on one at a time.\n",
    "#### The only problem is that it might be slower\n",
    "#### The second options is to fix all the sentences to a fixed length, with padding and maybe cutting long sentences, so batching will be possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ba44c-35b6-475d-9425-f7defb54e7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daf9150f-133d-45d3-bd60-b038b418ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc33ea-fa93-4e4a-bcb4-7610e6043c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5d192d9-c3c9-4c01-8472-62dd4dc3f24f",
   "metadata": {},
   "source": [
    "### The following is a copy paste implementation of the second method (it's not that slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a36ae027-ddd7-48c1-837b-aad688490552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, num_layers=1)\n",
    "\n",
    "        #self.hidden2tag = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tens_sent = torch.tensor(np.array(sentence)).float().to(device)\n",
    "        lstm_out, _ = self.lstm(tens_sent.view(len(sentence), 1, -1))\n",
    "        logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d39179ae-9238-4a35-a77e-f045befd863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calc_2(sentences, labels, model):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Turn off the gradients\n",
    "    with torch.no_grad():\n",
    "        # Loop through all of the validation set\n",
    "        for sentence, labels in zip(sentences, labels):\n",
    "            labels = torch.tensor(labels).float().to(device)\n",
    "            outputs = model(sentence).view(-1)\n",
    "            probs = nn.Sigmoid()(outputs)\n",
    "            probs[probs <= 0.3] = 0\n",
    "            probs[probs > 0.3] = 1\n",
    "            preds = probs.tolist()\n",
    "            #outputs[outputs>0] = 1\n",
    "            #outputs[outputs < 0] = 0\n",
    "            #preds = outputs.tolist()\n",
    "            labels = labels.tolist()\n",
    "            if len(preds) != len(labels):\n",
    "                print(len(preds), len(labels))\n",
    "            all_preds += preds\n",
    "            all_labels += labels\n",
    "    return round(f1_score(all_labels, all_preds), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e486ef36-cb3e-44e1-83ff-f9ffbacb03b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done\n",
      "train f1: 0.65\n",
      "test f1: 0.69\n",
      "epoch done\n",
      "train f1: 0.73\n",
      "test f1: 0.7\n",
      "epoch done\n",
      "train f1: 0.78\n",
      "test f1: 0.7\n",
      "epoch done\n",
      "train f1: 0.84\n",
      "test f1: 0.68\n",
      "epoch done\n",
      "train f1: 0.87\n",
      "test f1: 0.67\n",
      "epoch done\n",
      "train f1: 0.95\n",
      "test f1: 0.67\n",
      "epoch done\n",
      "train f1: 0.97\n",
      "test f1: 0.69\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(327, 200, 1)\n",
    "epochs = 7\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(4))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (sentence, sentence_labels) in enumerate(zip(train_sentences, train_labels)):\n",
    "        labels = torch.tensor(sentence_labels).float().to(device)\n",
    "        # Forward + Backward + Optimize\n",
    "        outputs = model(sentence).view(-1)\n",
    "        loss = criterion(outputs,labels)\n",
    "        #loss = \n",
    "        loss.backward()\n",
    "        if (i+1) % 2 == 0 or (i+1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        #if (i+1) % 300 == 0:\n",
    "            #precision, recall = f1_stats(outputs, labels)\n",
    "            #print(f\"precision is {precision} and recall is {recall}\")\n",
    "            #print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "            #       %(epoch+1, epochs, i+1,\n",
    "            #         len(train_dataset)//batch_size, loss.data))\n",
    "    print(\"epoch done\")\n",
    "    model.eval()\n",
    "    #acc = accuracy_calc(test_loader, our_net)\n",
    "    #print(acc)\n",
    "    print(f'train f1: {f1_calc_2(train_sentences, train_labels, model)}')\n",
    "    print(f'test f1: {f1_calc_2(dev_sentences, dev_labels, model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c910eda-4399-43db-8c66-bfe7ff1179b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
